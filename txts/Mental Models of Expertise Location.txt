521303 SGRXXX10.1177/1046496414521303Small Group ResearchEllwart et al.

research-article2014

Article

Team Mental Models 
of Expertise Location: 
Validation of a Field 
Survey Measure

Small Group Research
2014, Vol. 45(2) 119 –153
© The Author(s) 2014
Reprints and permissions:
sagepub.com/journalsPermissions.nav 
DOI: 10.1177/1046496414521303
sgr.sagepub.com

Thomas Ellwart1, Udo Konradt2,  
and Oliver Rack3

Abstract
This research provides and validates a field survey measure of team mental 
models (TMMs) on the location of team member expertise. The measure 
integrates two important aspects into the expertise location TMM Index: (a) 
the quality of meta-knowledge about experts within the team, and (b) team 
consensus  regarding  within-team  expertise.  Complementary  to  content-
specific  TMM  approaches,  this  measure  can  be  applied  across  different 
team and task types as a screening indicator in organizational surveys. To 
validate the TMM Index, an experimental study (n = 120, 40 teams) and a 
longitudinal field study (n = 130, 37 teams) were conducted. Both studies 
provide evidence that the TMM Index is a reliable screening indicator that 
corresponds to content-specific accuracy and consensus scores. Multilevel 
analyses revealed that the TMM Index predicts team performance (self- and 
other ratings), team coordination, and individual variables such as knowledge 
credibility and self-efficacy over time.

Keywords
team  cognition,  team  mental  models,  transactive  memory  systems, 
performance

1University of Trier, Germany
2University of Kiel, Germany
3University of Applied Sciences Northwestern Switzerland, Olten, Switzerland

Corresponding Author:
Thomas Ellwart, University of Trier, Universitaetsring 15, Trier, D-54296, Germany. 
Email: ellwart@uni-trier.de

120 

Small Group Research 45(2)

Knowledge and skills of team members are essential prerequisites for numer-
ous team processes and outcomes, including team functioning, team develop-
ment,  and  team  performance  (Kozlowski  &  Chao,  2012).  Team  members’ 
individual expertise refers to “specialized skills and knowledge individuals 
bring to the team’s task” (Faraj & Sproull, 2000, p. 1555). In addition and 
complementary  to  individual  expertise,  the  understanding  of  “who  knows 
what in the team” is another important perspective and key focus of this arti-
cle.  The  knowledge  about  how  expertise  is  distributed  within  the  team  or 
knowledge about expertise location is defined as a meta-knowledge about the 
variety of potentially useful expertise sources in terms of other team mem-
bers’  knowledge  and  skills  (Faraj  &  Sproull,  2000).  Past  research  demon-
strated positive effects of high quality knowledge on expertise location, both 
on  individual-  and  team-level  processes  (e.g.,  Austin,  2003;  Ellwart, 
Buendgens, & Rack, 2014; Hollingshead, 1998). However, outside the lab, 
the measurement of expertise location on the team level is lacking valid and 
economic measurement scales for the organizational context. Thus, this arti-
cle aims to develop and validate a field measure that helps to evaluate and to 
improve this type of team knowledge in field settings.

A prominent theory to represent knowledge about expertise location refers 
to team mental models (TMMs), which are collectively shared mental repre-
sentations  of  key  elements  of  the  team’s  relevant  environment  (Klimoski  & 
Mohammed,  1994).  Empirical  studies  have  shown  positive  relationships 
between TMM and different outcome variables (for overviews, see DeChurch 
& Mesmer-Magnus, 2010a, 2010b; Mohammed, Ferzandi, & Hamilton, 2010). 
TMMs are broadly categorized into two content domains: (a) taskwork mental 
models referring, for example, to shared knowledge about tools, strategies, or 
plans;  and  (b)  teamwork  mental  models  referring,  for  example,  to  shared 
knowledge  about  teammates’  skills  or  interaction  requirements  (Cannon-
Bowers, Salas, & Converse, 1993; Mohammed et al., 2010). For the purpose of 
this study, we speak about TMMs of expertise location that belong to the spe-
cific content domain of teamwork mental models, and current literature under-
lines  the  added  value  of  operationalizing  specific  content  domains  in TMM 
research (Mohammed et al., 2010; Rentsch & Mot, 2012). To operationalize 
TMM  of  expertise  location,  we  differentiate  between  the  two  aspects  of  (a) 
quality of knowledge and (b) consensus as the degree of sharedness between 
the team member perceptions. Thus, team members not only need to know who 
is  expert  in  the  team  (quality  of  knowledge),  there  must  also  be  consensus 
about the expertise location among the team members.

TMM  research  has  suggested  several  methods,  processes,  and  tools  to 
capture TMMs, which differ in content, structure, and accuracy (DeChurch & 
Mesmer-Magnus,  2010b;  Ellwart,  Biemann,  &  Rack,  2011;  Mohammed  
et al., 2010). Although existing TMM measures capture the team’s cognitions 

Ellwart et al. 

121

thoroughly, they nevertheless have some limitations. First, most of the cur-
rent TMM measures are context dependent in nature (Lewis, 2003; Rentsch 
& Mot, 2012) and, therefore, “need to be tailored to the specific task under 
investigation”  (Mohammed  et  al.,  2010,  p.  890).  However,  organizational 
teams hardly ever work on tasks comprising straightforward domain-specific 
characteristics as duties and team responsibilities typically vary across teams 
(Ellwart et al., 2011; Lewis, 2003). As a consequence, TMM measures on 
expertise location cannot be compared across teams. Second, because avail-
able measures require that the specific domain(s) of expertise be examined in 
a lengthy and time-consuming analysis of knowledge, it might prevent orga-
nizations from using them. For example, in applications such as organiza-
tional change settings (Burke, 2011) or in action research (Chin, Benne, & 
Bennis, 1985), numerous teams are surveyed to monitor or provide feedback 
on team processes and states. As a consequence, most measures are used in 
experimental  and  controlled  settings,  although  deficits  in  knowledge  of 
expertise location by early screening measures might be highly valuable.

In this article, we propose and validate a measure of TMM of expertise loca-
tion—the TMM Index. Most previous measures only reflect whether there is 
consensus among the team members’ mental model (high vs. low), but ignored 
whether team members have an elaborate mental representation (i.e., high con-
sensus in not knowing vs. high consensus in knowing). Thus, the TMM Index 
of  expertise  location  integrates  the  quality  of  the  members’  knowledge  of 
within-team  experts  (meta-knowledge  of  expertise  location),  and  the  team’s 
consensus on perceived expertise location within a single coefficient. By pro-
viding a measure that is task and team independent, applicable across diverse 
teams, and easily employed, this research contributes to both team cognition 
research and to managerial practice. Furthermore, this measure also adds to 
TMM  research  by  referencing  TMM  not  in  a  generic  or  abstract  way 
(Mohammed et al., 2010) but in the specific content domain of team members’ 
knowledge  about  expertise  location.  In  two  studies,  we  provide  evidence  to 
support our assumptions. Study 1 was constructed to show that the TMM Index 
is  a  sensitive  and  accurate  indicator  of  knowledge  of  expertise  location  and 
team consensus. Study 2 examined the predictive validity of the TMM Index in 
organizational teams over time in a field setting.

Knowing Who Knows What: Knowledge of Within-
Team Expertise Location
TMMs of Expertise Location
Within the broad label of team cognition (Salas & Fiore, 2004; Salas, Fiore, 
&  Letsky,  2012),  the  concept  of  TMMs  (DeChurch  &  Mesmer-Magnus, 

122 

Small Group Research 45(2)

2010a,  2010b;  Mohammed  et  al.,  2010)  describes  the  shared,  organized 
understanding of knowledge relevant to key elements of the team (teamwork 
mental models) and its task(s) (taskwork mental models; Cannon-Bowers et 
al., 1993; Klimoski & Mohammed, 1994).1 It is important to note that task-
work  and  teamwork  mental  models  are  generic  categories  (Mohammed, 
Tesler, & Hamilton, 2012), and researchers need to specify the knowledge 
domain of interest. While taskwork mental models refer to characteristics of 
the  team’s  work,  major  task  duties,  equipment,  and  resources,  teamwork 
mental models include features of how team members interact and about their 
roles, responsibilities, and knowledge domains, which facilitate team interac-
tions to accomplish team goals (Marks, Zaccaro, & Mathieu, 2000; Mathieu, 
Heffner, Goodwin, Salas, & Cannon-Bowers, 2000). Following this categori-
zation, the knowledge about the location of expertise belongs to the category 
of teamwork mental models and represents a more precise conceptualization 
and operationalization of TMM. Thus, TMM of expertise location represents 
a specific type of TMM, and as recommended by Mohammed et al. (2012), 
they move away from the abstract content domains of TMM while acknowl-
edging their multifaceted nature and complexity (see Table 1).

Following the identification of the content domain, the type of sharing of 
this knowledge represents a further major property of TMM (Mohammed et 
al., 2010; Rentsch & Mot, 2012). In TMM research, indices of sharedness 
indicate how the individual team members share the knowledge of interest 
(Mathieu, Heffner, Goodwin, Cannon-Bowers, & Salas, 2005). In addition, 
various forms of sharedness are distinguished in TMM research. For exam-
ple,  perceptual  approaches  measure  the  quality  of  the TMM  content  (e.g., 
level/quality of knowledge about expertise location most often measured by 
rating  scales)  and  focus  on  the  teams’  consensus  in  terms  of  within-group 
agreement. Alternatively, structural approaches focus on within-team simi-
larity or accuracy of cognitive team structures (e.g., representations of spe-
cific  team  experts  and  their  areas  of  expertise)  by  modeling  how  specific 
contents  are  organized  in  the  participant’s  mind  (Mohammed  et  al.,  2010; 
Rentsch & Mot, 2012). Thus, different forms of sharedness indicators result 
in different types of methodology and measurement (Rentsch & Mot, 2012).
In this article, the TMM measure will be perceptual in nature, quantifying 
(a) the quality of the members’ knowledge of within-team experts (expertise 
location) and (b) the team’s consensus on perceived expertise location into 
one TMM Index. This perceptual approach, based on rating scales, will not 
allow identification of the structure and accuracy of a TMM. However, as 
Rentsch and Mot (2012) point out, the perceptual versus structural approach 
“traded off information about content or structure in favor of the other and 
both are lacking to some degree” (p. 150). In this vein, our approach does not 

 

 

 

 

 

 

 

 

 

5
8

.

 

 

 

 

 

 

 

 

 

2
9

.

*
*
2
4

.

3
9

.

*
*
3
6

.

*
*
6
4

.

—

2
0

.

9
1
−

.

5
2
−

.

8

7

—

6
1
−

.

8
1

.

3
1

.

5
2

.

6

—

1
1

.

4
0
−

.

*
1
3
−

.

*
1
3
−

.

†
9
2
−

.

9
8

.

8
0
−

.

2
1

.

9
0
−

.

*
*
5
6

.

*
*
2
8

.

†
0
3

.

9
1

.

6
0

.

*
*
8
4

.

†
7
2

.

†
9
2

.

5

4

3

3
8

.

1
9

.

*
4
3

.

3
8

.

7
1

.

1
2

.

1
8

.

6
2

.

*
*
9
5

.

*
*
4
5

.

*
*
6
6

.

4
8

.

7
1

.

*
*
5
7

.

*
7
3

.

—

8
0

.

9
0
−

.

4
0
−

.

6
0
−

.

3
1

.

0
1

9

8

7

6

5

4

3

2

1

.

i

2
 
d
n
a
 
1
 
s
e
d
u
t
S
 
n
i
 
s
n
o
i
t
a
l
e
r
r
o
c
r
e
t
n

I
 
d
n
a
 
,
s
n
o
i
t
a
i
v
e
D
 
d
r
a
d
n
a
t
S
 
,
s
n
a
e
M

 
.
1
 
e
b
a
T

l

*
1
3

.

—

*
*
7
7

.

0
1

.

—

*
*
2
6

.

*
*
7
6

.

2
9

.

8
1

.

*
6
3
−

.

*
1
3
−

.

*
*
6
4
−

.

2
1

.

†
9
2

.

0
1

.

2

†
5
3

.

*
3
4

.

—

2
1

.

4
1

.

*
5
4

.

4
0

.

*
*
2
3

.

*
*
0
4

.

†
8
2

.

*
*
0
5

.

*
*
5
4

.

*
*
6
4

.

*
*
4
4

.

*
*
9
4

.

*
*
1
4

.

†
1
3

.

2
9

.

1

1
2

.

9
1
−

.

7
0

.

2
1
−

.

D
S

1
3
1

.

2
2
0

.

5
5
1

.

4
8
0

.

4
6
2

.

5
2
6

.

5
3
0

.

5
7
0

.

8
6
0

.

3
6
0

.

D
S

9
7
0

.

1
1
0

.

3
3
0

.

4
5
0

.

6
5
0

.

0
8
0

.

8
4
0

.

9
3
0

.

M

9
4

.

1
7
3

.

3
2
3

.

5
9
5

.

7
7
9

.

1
2
2
3

.

5
6
1

.

8
5
5

.

5
0
6

.

4
2
5

.

M

2
6

.

7
3
3

.

5
0
2

.

6
7
3

.

8
9
3

.

6
7
3

.

7
0
4

.

2
8
3

.

s
u
s
n
e
s
n
o
c
 
c
i
f
i
c
e
p
s
-
k
s
a
T

 
.
2
 

y
c
a
r
u
c
c
a
 
c
i
f
i
c
e
p
s
-
k
s
a
T

 
.
3
 

s
g
n
i
t
a
r
-
f
l
e
S
 
:
n
o
i
t
a
n
d
r
o
o
C

i

 
.
4
 

s
e
g
a
s
s
e
M

 
:
n
o
i
t
a
n
d
r
o
o
C

i

 
.
5
 

s
g
n
i
t
a
r
-
f
l
e
S
 
:
e
c
n
a
m
r
o
f
r
e
P
 
.
8
 

y
t
i
l
a
u
Q

 
:
e
c
n
a
m
r
o
f
r
e
P
 
.
7
 

e
m
T

i

 
:
e
c
n
a
m
r
o
f
r
e
P
 
.
6
 

1
 
y
d
u
t
S
 
:
e
b
a
i
r
a
V

l

x
e
d
n

I
 

M
M
T

 
.
1
 

y
t
i
l
i

i

b
d
e
r
c
 
e
g
d
e
w
o
n
K

l

 
.
9
 

)
2
T
(
 
s
g
n
i
t
a
r
-
f
l
e
S
 
:
n
o
i
t
a
n
d
r
o
o
C

i

 
.
4

)
2
T
(
 
s
g
n
i
t
a
r
-
f
l
e
S
 
:
e
c
n
a
m
r
o
f
r
e
P
 
.
5

)
2
T
(
 
s
g
n
i
t
a
r
 
r
o
t
u
T

 
:
e
c
n
a
m
r
o
f
r
e
P
 
.
6

)
1
T
(
 
s
u
s
n
e
s
n
o
c
 
c
i
f
i
c
e
p
s
-
k
s
a
T

 
.
2

)
1
T
(
 
y
c
a
r
u
c
c
a
 
c
i
f
i
c
e
p
s
-
k
s
a
T

 
.
3

)
1
T
(
 
x
e
d
n

I
 

M
M
T

 
.
1

2
 
y
d
u
t
S
 
:
e
b
a
i
r
a
V

l

y
c
a
c

i
f
f
e
-
f
l
e
S
 
.
0
1

y
t
i
l
i

i

b
d
e
r
c
 
e
g
d
e
w
o
n
K

l

 
.
7

y
c
a
c

i
f
f
e
-
f
l
e
S
 
.
8

 
y

l
r
a
e
 
=
 
1
T

 
;
)
3
0
0
2
(
 
n
i
t
s
u
A
 
o
t
 
g
n
d
r
o
c
c
a
 
d
e
t
a
l
u
c

i

l
a
c
 
e
r
a
 
y
c
a
r
u
c
c
a
 
d
n
a
 
s
u
s
n
e
s
n
o
c
 
c

i
f
i
c
e
p
s
-
k
s
a
T

 
.
)
s

m
a
e
t
 
7
3
 
=
 
n
(
 
2
 
y
d
u
t
S
 
,
)
s

m
a
e
t
 
0
4
 
=
 
n
(
 
1
 
y
d
u
t
S
 
.
e
t
o
N

 
.
l
a
n
o
g
a
i
d
 
e
h
t
 
n
i
 
e
p
y
t
 
e
c
a
f
d
o
b
 
n
i
 
d
e
t
n
i
r
p
 
s
i
 
a
h
p
l
a
 
t
n
e
c

i

l

i
f
f
e
o
c
 
,
s
e
l
a
c
s
 
r
o
F
 
.
2
 
y
d
u
t
S
 
n
i
 
t
c
e
o
r
p
 
e
h
t
 
f
o
 
e
s
a
h
p
 
r
e
t
a
l
 

j

=
 
2
T

j

 
;
2
 
y
d
u
t
S
 
n
i
 
t
c
e
o
r
p
 
e
h
t
 
f
o
 
e
s
a
h
p

.

d
e

l
i
a
t
-
o
w

t
 
d
e
t
r
o
p
e
r
 
e
r
a
 
s
e
c
n
a
c

i
f
i

n
g
i
S

.

1
0
.
 

<
 
p
*
*
 
.
5
0
.
 

<
 
p
*
 
.
0
1
.
 

<
 
p
†

123

124 

Small Group Research 45(2)

rule  out  the  structural  measures  of  TMM  for  application  and  research  but 
rather serves as a complement because it is applicable for specific context 
situations and samples.

Measurement of TMM of Expertise Location in 
Field Settings
As  pointed  out  above,  this  article  aims  to  develop  a  measure  of TMM  on 
expertise location that can be applied in field settings as a first indicator or 
screening about the quality and the consensus of the teams’ knowledge about 
expertise location. The applied setting and sample focused on here require 
that the following characteristics are met. First, the measure does not depend 
on a specific team or task and can be applied across settings independently 
from  the  specific  areas  of  expertise  within  the  teams  surveyed.  Second, 
because screening measures often serve the purpose of initial data collection 
in  change  processes  and  are  often  applied  in  the  context  of  larger  surveys 
(Burke, 2011), the measure developed in this study was designed to require a 
low investment of resources (time and effort) for its completion. Third, in 
contrast  to  the  existing  measures  of  individual  knowledge  about  expertise 
location (e.g., Faraj & Sproull, 2000), the TMM measure has to capture the 
quality of knowledge about expertise location as well as the consensus on it 
at the team level. However, most of the measurement approaches of TMM 
have been successfully applied in highly standardized experimental and task-
specific  settings  (e.g.,  Cooke,  Salas,  Cannon-Bowers,  &  Stout,  2000; 
DeChurch  &  Mesmer-Magnus,  2010b;  Ellwart  et  al.,  2011;  Langan-Fox, 
Code,  &  Langfield-Smith,  2000;  Mohammed  et  al.,  2010;  Mohammed, 
Klimoski, & Rentsch, 2000), underlining the importance of methods that are 
applicable for research in the field context.

To  assess  TMM,  two  methodological  approaches  can  be  distinguished: 
structure- and perception-oriented measures (Cooke et al., 2000; DeChurch 
&  Mesmer-Magnus,  2010b;  Langan-Fox  et  al.,  2000;  Mohammed  et  al., 
2010; Mohammed et al., 2000; see also Table 1).

Structure-Oriented Measures
The first methodological approach focuses on the analysis of the structure 
between various elements of the TMM by attempting to measure the pattern 
of knowledge arrangement. Structured cognition is often assessed using tech-
niques  such  as  pathfinder  networks  (Stout,  Cannon-Bowers,  Salas,  & 
Milanovich,  1999),  UCINET  (Mathieu  et  al.,  2005),  concept  mapping 

Ellwart et al. 

125

(Marks, Sabella, Burke, & Zaccaro, 2002), and multidimensional scaling (cf. 
Cooke et al., 2000; Mohammed et al., 2000). Researchers ask for information 
about how team members relate specific concepts of knowledge to each other 
(e.g., expert A is related to knowledge Y but less related to knowledge X); 
these  comparisons  are  transferred  statistically  into  indicators  of  similarity. 
The  overlap  or  agreement  of  individual  structures  indicates  whether  team 
members  have  a  similar  “network  of  concepts  in  mind”  (Rentsch  &  Mot, 
2012,  p.  151).  A  second  indicator  of  structural  methods  is  accuracy.  As 
Rentsch and Mot (2012) explain, accurate structured “cognitions exist to the 
extent cognitions match a target” (p. 151). In this vein, accuracy depends on 
the comparison with a correct structured model, and researchers have to iden-
tify in advance the perfect structure to which the TMM are to be compared.

Overall, the emphasis of structured TMM is on the recognition, structural 
relation, and accuracy of specific concepts. Transferred to the measurement 
of expertise location in the field, specific domains of expertise would have to 
be determined prior to the measurement, which hinders the early application 
as  a  screening  instrument  across  various  types  of  teams  and  task  types. 
Moreover, structural TMM do not operationalize the degree with which team 
members describe the quality or level of a specific concept (e.g., how much 
knowledge about an expertise location exists). This focus on the quality of 
perceptions and its sharedness is operationalized in perceptual conceptualiza-
tions of TMM.

Perception-Oriented Measures
The second methodological approach is focused on beliefs or perceptions of 
team  members  (DeChurch  &  Mesmer-Magnus,  2010b).2  As  proposed  by 
Rentsch, Small, and Hanges (2008), this approach will not afford an under-
standing of explanatory relations. Using Likert-type scales, researchers mea-
sure two forms of perceptual information: (a) quality in terms of the mean 
group perception (e.g., do all team members know the specific expertise of 
each  other)  and  (b)  consensus  between  the  ratings  of  the  team  members 
(Rentsch & Mot, 2012). In early studies, indices are based on the concept of 
within-group agreement (e.g., rWG, see James, Demaree, & Wolf, 1984) and 
are  used  to  derive TMM  agreement  (e.g.,  Eby,  Meade,  Parisi,  &  Douthitt, 
1999; Levesque, Wilson, & Wholey, 2001; Webber, Chen, Payne, Marsh, & 
Zaccaro, 2000). However, there are strong limitations and problems in inter-
preting the value of rWG -related computations (magnitude of the rWG depends 
on scale anchors and sample size; negative values are set at zero; see Brown 
& Hauenstein, 2005). Transferred to our research, the perceptual approach 
can be applied in the field across different types of teams and tasks. Imagine, 

126 

Small Group Research 45(2)

for instance, a survey item such as “In your team, do you know who pos-
sesses specific expertise?” Agreement indices within each team would serve 
as indicator of consensus (i.e., degree to which all members share the same 
quality  of  knowing  team  experts).  However,  it  remains  unclear  whether 
agreement or disagreement was in knowing the experts (high level or good 
quality of knowledge) or not  knowing the experts (low level or quality of 
knowledge) because agreement measures ignore information about the mean 
level of the ratings. Because of this limitation, the approach developed in this 
article integrates quality and consensus into one perceptual TMM Index.

Operationalization and Validation of the TMM 
Index of Expertise Location
To  assess  this  variable,  we  adopted  Faraj  and  Sproull’s  (2000)  measure 
because  it  represented  a  validated  field  measure  with  a  focus  on  expertise 
location.  Originally,  this  measure  reflects,  from  a  team-level  perspective, 
whether the team has knowledge about the experts within the team. Because 
of its group-focused survey wording, the items do not mirror individual rep-
resentation of each member’s knowledge (cf. Klein, Conn, Smith, & Sorra, 
2001). Thus, the original items were changed to an individual wording per-
spective.3  From  this  perspective,  the  measure  captures  the  quality  of  indi-
vidual meta-knowledge about expertise location (e.g., Ellwart et al., 2011; 
Ellwart & Konradt, 2007; Srivastava, Bartol, & Locke, 2006). For reasons of 
simplicity, in the remaining part of the article, the term meta-knowledge will 
be used to refer to the quality of knowledge about expertise location.

Following the idea of TMM, we argue that consensus on individual exper-
tise perceptions is a team phenomenon related to a specific type of perceptual 
TMM  (cf.  Cannon-Bowers  et  al.,  1993;  DeChurch  &  Mesmer-Magnus, 
2010b; Mohammed et al., 2010). Consensus is conceptualized as the extent 
of agreement on perceptions of expertise location within the team (Rentsch & 
Mot, 2012). To calculate the degree of consensus of the individual scores at 
the team level, we used the average deviation (AD) score (Burke & Dunlap, 
2002; Burke, Finkelstein, & Dusig, 1999). Compared with the rWG-related 
indices applied as the agreement score in the early studies on TMM (e.g., Eby 
et al., 1999; Levesque et al., 2001; Webber et al., 2000), the AD overcomes 
the shortcomings of the rWG (e.g., scale dependency, interpretation of nega-
tive  values,  assumption  of  a  null  distribution  as  agreement;  see  Brown  & 
Hauenstein, 2005, for an overview).4 As the aim of the TMM Index of exper-
tise location is to integrate individual knowledge and team consensus into a 
single  score,  the  AD  score  is  subtracted  from  the  mean  score  to  provide 

Ellwart et al. 

127

information  about  the  team  members’  knowledge  of  expertise  location  in 
relation to the degree of consensus within the team.

This  type  of  TMM  measure  is  clearly  perceptual  in  nature  (Rentsch  & 
Mot,  2012)  and  has  the  advantage  of  screening  a  very  specific  domain  of 
team cognition while demanding only little time and effort of respondents. 
However, the screening approach will neither reveal the underlying structure 
of the representation of team expertises (who exactly is the expert in specific 
domains) to researchers and practitioners nor indicate whether the percep-
tions are accurate. This information requires more elaborated techniques such 
as structural measures. The value of this specific TMM Index is its practica-
bility as an early screening measure independent of the specific tasks or team 
characteristics.  Moreover,  in  the  context  of  change  and  action  research 
(Burke, 2011), it offers a first insight into the teams’ cognitions within the 
framework  of  large-scale  surveys  and  identifies  those  units  which  are  of 
interest for further structural diagnoses of TMMs.

To provide validity of this approach, the TMM Index of expertise location 
should be sensitive to (a) team differences regarding high and low quality of 
knowledge about the experts in the team, and (b) high and low team consen-
sus on this perception. Thus, this perceptual TMM Index must relate to the 
objective underlying knowledge about team experts and its consensus among 
team members. If the underlying knowledge about team experts and its con-
sensus among team members is experimentally manipulated in ad hoc teams 
(by creating different expertise domains and forming different heights of con-
sensus about the perception of expertise), the TMM Index would yield valid-
ity  when  it  can  differentiate  between  these  experimentally  generated 
conditions. This experimental validation allows creating teams with high ver-
sus  low  qualities  of  knowledge  about  expertise  location  as  well  as  groups 
with  high  versus  low  consensus. The  newly  developed  measure  should  be 
able  to  identify  the  manipulated  condition.  In  a  field  setting,  validation 
depends on the given variance of TMM in given groups and requires inten-
sive team and task analysis to objectively identify the knowledge and consen-
sus about specific team experts and their domains of expertise. Thus, in Study 
1,  knowledge  and  consensus  about  expertise  location  was  experimentally 
manipulated between groups to test the sensitivity of the TMM Index toward 
the objective underlying knowledge structure. Specifically, it was proposed,

Hypothesis 1: The TMM Index differentiates between teams with high 
knowledge and high consensus (highest indices) versus high knowledge 
and low consensus as well as between teams with low knowledge and low 
consensus and low knowledge and high consensus (lowest indices).

128 

Small Group Research 45(2)

The TMM Index of expertise location reflects team members’ general per-
ceptions on whether they have knowledge of team members’ expertise and 
skills and whether there is consensus about this perception within the team. 
To prove content validity, the task-unspecific perception in the TMM Index 
(e.g., “I know which team members have expertise in specific areas”) should 
relate  to  measures  that  capture  accuracy  and  consensus  of  perception  in  a 
task-specific way by identifying and naming experts and their related exper-
tise domains. Austin (2003) proposed an elaborate method to capture task-
specific  accuracy  and  consensus  of  expertise  location.  Each  team  member 
would identify and name one expert of several task-specific areas (e.g., sta-
tistics, writing). These named experts are then compared and integrated into 
a team consensus score. Moreover, each participant would rate his or her own 
expertise in the mentioned areas, offering the possibility to calculate accu-
racy scores on whether members identified themselves as an expert in the 
same areas (for details, see Austin, 2003). A clear indication of content valid-
ity of our task and team independent TMM Index of expertise location would 
be  provided  if  it  corresponds  with  the  accuracy  and  consensus  from  task-
specific direct expertise ratings. Consequently, we propose,

Hypothesis  2a:  The  TMM  Index  is  positively  related  to  task-specific 
accuracy scores from expertise ratings.
Hypothesis 2b: The TMM Index is positively related to task-specific con-
sensus scores from expertise ratings.

It is obvious that accuracy and consensus ratings by team members depend 
on the validity of the self-evaluation of the experts and may be biased in some 
ways. For this reason, Hypothesis 1 was formulated to test the sensitivity of 
the TMM Index related to the experimentally manipulated objective knowl-
edge structure in the groups, and Hypotheses 2 was designed to compare the 
task-unspecific  TMM  Index  with  task-  and  team-specific  evaluations  of 
expertise domains.

The TMM Index of expertise location should also be positively related to 
team process and outcome variables proposed in the research on TMM and 
transactive  memory  systems  (DeChurch  &  Mesmer-Magnus,  2010a; 
Mohammed et al., 2010). On the team level, pertinent research has shown 
that  shared  knowledge  about  the  location  of  expertise  within  the  team 
improves coordination and performance compared with teams with little con-
sensus and meta-knowledge (e.g., Austin, 2003; Levesque et al., 2001; Lewis, 
2003). Coordination as an outcome represents the extent to which team mem-
bers  have  managed  interdependencies  to  varying  degrees  of  success 
(Espinosa, Lerch, & Kraut, 2004; Hoegl & Gemuenden, 2001). At the team 

Ellwart et al. 

129

level, the mental models of expertise location should be positively related to 
team coordination (Lewis, 2003) because the TMM allows team members to 
integrate knowledge, ask questions, and search for missing knowledge in an 
efficient fashion (Moreland & Myaskovsky, 2000). The concept of TMM was 
initially  developed  to  explain  performance  differences  between  groups 
(Mohammed et al., 2010), and studies with perceptual and structural TMM 
showed  the  positive  relationships  with  different  performance  indicators 
(DeChurch & Mesmer-Magnus, 2010b). Indicators for performance are het-
erogenic and reach from scoring points in experimental tasks, decision qual-
ity, efficiency, or satisfaction (Kozlowski & Ilgen, 2006; Mohammed et al., 
2010). Thus, we propose on the team level,

Hypothesis 3a: The TMM Index of expertise location is positively related 
to coordination success.
Hypothesis 3b: The TMM Index of expertise location is positively related 
to team performance.

High quality and consensus of knowledge about the teams’ experts will 
also affect the individual team member in her or his task-related thinking and 
behavior. Recent work showed positive effects of TMM contents on individ-
ual variables such as members’ trustworthiness (Chou, Wang, Wang, Huang, 
& Cheng, 2008). Specifically, the TMM on expertise location should increase 
the team members’ trust to rely on the expertise of others (i.e., knowledge 
credibility; Lewis, 2003). Moreover, when team members know who to ask 
in difficult situations and are aware of their own expertise roles, the personal 
beliefs to have the ability and the resources to perform the task well (self-
efficacy)  will  also  increase  (Bandura,  1977,  1997;  Peterson,  Mitchell, 
Thompson, & Burr, 2000). Thus, we propose on the individual level,

Hypothesis  4a:  Team  members  in  teams  with  high  TMM  Index  will 
exhibit higher scores of knowledge credibility.
Hypothesis  4b:  Team  members  in  teams  with  high  TMM  Index  will 
exhibit higher scores of task-related self-efficacy.

Two  studies  were  conducted  to  test  the  validity  of  the  TMM  Index  of 
expertise location. In Study 1, quality (meta-knowledge) and team consensus 
on expertise location were experimentally manipulated. The aim was to pro-
vide evidence that the TMM Index is a sensitive and valid indicator of TMMs 
of expertise location, and that the TMM Index relates to individual-and team-
related outcome variables. The experimental manipulation allows controlling 
the objective knowledge allocation and recognition as well as consensus on 

130 

Small Group Research 45(2)

within-team location of expertise to test the sensitivity of the index. Identical 
scales  and  measurement  approaches  of  the  first  study  were  applied  in  the 
longitudinal field study (Study 2). The goal of Study 2 was to show that the 
TMM Index is a valid measure for survey research that predicts individual- 
and team-level outcomes of ongoing teams over time.

Study 1
Method
Sample.  One  hundred  twenty  undergraduate  students  (30  males  and  90 
females, M age = 24 years, SD = 6.1 years) were recruited from a large Ger-
man public university to participate for course credit. Participants worked in 
three-person teams, and the composition of the teams did not differ signifi-
cantly with regard to sex and age (Fs < 1, ns).

Procedure and design.  In Study 1, the objective amount of individual meta-
knowledge about team experts was varied in a way that mirrors how teams 
may  differ  in  the  level  of  knowing  the  team  experts  (high  and  low  meta-
knowledge) as well as in the consensus of this perception (high and low con-
sensus). Accordingly, a team task was developed which was based on the idea 
of the hidden profile paradigm (Stasser, 1992; Stasser, Stewart, & Witten-
baum, 1995) because of the advantage it offers to manipulate high and low 
levels  of  knowledge  and  team  consensus  in  a  2  ×  2  experimental  design. 
Knowledge about the expertise location in the team was systematically var-
ied between the experimental teams of three participants who were assigned 
to solve a decision-making task. Participants, who were randomly assigned to 
the teams, were told that they worked in a company that analyzes weather 
information to recommend one of three travel routes to their customers. Each 
member received a specific customer request regarding three possible travel 
routes, each consisting of three stations. Based on weather criteria given by 
the customers (e.g., customer 1: warm, dry, calm weather for swimming; cus-
tomer  2:  mild,  dry,  windy  weather  for  sailing),  each  team  member  had  to 
decide which of the three routes was best suited to the customers’ wishes. 
Although  the  customer  requests  differed  for  each  team  member,  the  team 
worked and made decisions using the identical weather information. How-
ever, to create interdependence between the team members, relevant weather 
information  from  the  weather  domains  was  distributed  between  domain 
experts (e.g., expert for wind, temperature, or rain). Each expert had all infor-
mation about the specific weather domain at all stations along the routes. To 
gather the missing information from the other experts, participants had the 

Ellwart et al. 

131

chance to exchange information. Thus, each team member had to solve the 
task  individually  but  needed  information  from  the  other  experts  (resource 
interdependence;  cf. Wageman,  1995). The  task  had  a  correct  solution  for 
each customer request based on the best combination of customers’ condi-
tions and the actual weather situation for each route and station.

In the resulting 2 × 2 experimental design, four experimental groups were 
tested:  (a)  teams  with  high  quality  of  knowledge  about  expertise  location 
(meta-knowledge) and high team consensus (all participants knew that each 
member received information about the expertise domains of each other team 
member); (b) teams with low meta-knowledge and high consensus (no one 
received information about the expertise of the other team members and all 
were told that no one else received this information either); (c) teams with 
high meta-knowledge and low consensus (two participants received informa-
tion about the expertise within the team but no one was given information 
about what other team members were/were not told); and (d) teams with low 
meta-knowledge and high consensus (only one of the three team members 
received expertise information but all were informed that only one member 
received said information and were told which member). Thus, meta-knowl-
edge and consensus in the team depended both on the distribution of exper-
tise and the accuracy of knowledge of location of expertise.

Each group had to solve two sets of three customer requests in two subse-
quent tasks. Although content and weather conditions changed between the 
tasks, the expertise domains did not. At the start of the first request, partici-
pants  received  instructions  and  information  about  expertise  (depending  on 
the experimental manipulation). Following the manipulation check, each par-
ticipant worked on the specific task to determine what information was miss-
ing. To document the questioning procedure for missing information, and to 
provide data with which coordination costs could be operationalized, each 
team member was asked to write a simulated email to the team members. 
Emails were either addressed to all members representing high coordination 
costs versus addressed to specific team members representing low coordina-
tion costs. After completion of the email task, missing information was per-
sonally  exchanged  among  the  group  participants  in  face-to-face  meetings, 
and  participants  finished  the  first  request  and  completed  the  items  of  the 
TMM Index measure (predictor at T1). This was followed by objective exper-
tise ratings (see Austin, 2003) where participants named the experts of the 
different weather domains and rated their own expertise. Then, participants 
worked on the second set of customer requests, and the information exchange 
followed. Finally, participants completed questionnaires on coordination suc-
cess, team performance, knowledge credibility, and self-efficacy at the end of 
the experimental session (criteria at T2).

132 

Small Group Research 45(2)

TMM Index.  This measure was adopted from Faraj and Sproull (2000; see 
also Ellwart et al., 2014; Srivastava et al., 2006) and consisted of four items 
(“I have a good ‘map’ of other team members’ talents and skills,” “I know 
which team members have expertise in specific areas,” “I know what task-
related  skills  and  knowledge  each  team  member  possesses,”  and  “I  know 
who on the team has specialized skills and knowledge that are relevant to 
me”). All items of the TMM Index of expertise location, perceived perfor-
mance, coordination success, knowledge credibility, and self-efficacy were 
measured using a 7-point rating measure ranging from 1 (strongly disagree) 
to 7 (strongly agree). Cronbach’s alpha = .92. To calculate consensus, AD 
scores for all items of the measure were computed (Burke et al., 1999). This 
represents the mean average absolute deviation divided by the group mean 
of each item. Finally, AD scores were averaged for the whole measure (cf. 
Burke et al., 1999). To calculate the TMM Index, AD was subtracted from the 
teams’ mean score of the measure.

Task-specific consensus and accuracy.  For the validation of the TMM Index, 
task-specific  consensus  and  accuracy  scores  were  calculated  based  on  the 
identification of each expert and experimental assignment (cf. Austin, 2003). 
Each team member had to name who was the team expert for each of the 
six domains (e.g., condensation, cloudiness, wind, air temperature, humidity, 
and visibility). Teams whose members identified the same individual as an 
expert on a given domain were considered to have high task consensus. Stan-
dard deviation scores were calculated to measure team member consensus of 
expert identification for each domain. Finally, domain consensus scores were 
combined to a single mean consensus score for the team. Thereby, a higher 
score indicated higher team consensus.

Accuracy was measured by combining experimentally assigned expertise 
and team members’ objective expertise ratings of expertise location. If the 
identified experts concurred with the assigned expertise, team’s accuracy is 
considered high. Individual-level perceived expertise accuracy scores were 
averaged to the team level to create the team accuracy score (for details on 
the computation, see Austin, 2003).

Coordination.  Good coordination is given when members can efficiently 
ask questions and search for missing knowledge (Moreland & Myaskovsky, 
2000). Email messages formulated in response to the task of documenting the 
questioning procedure for missing information were used to assess coordina-
tion. The emails could be addressed either to all members or to a specific 
person in the team. Participants were instructed to communicate, in a clear 
and  brief  manner,  only  task-relevant  information  (questions  about  weather 

Ellwart et al. 

133

conditions). The total number of emails each team member would have to 
answer during a task represented coordination costs. Perceived coordination 
success  was  measured  at  the  end  of  the  experimental  session  with  a  four-
item measure from Lewis (2003; for example, “Our team works together in 
a well-coordinated fashion.”). Cronbach’s alpha = .89. Previous research has 
demonstrated that the measure shows good psychometric properties (Ellwart 
& Konradt, 2007; Lewis, 2003).

Team performance.  Team performance was measured quantitatively using 
the time needed to solve the final task and the correct decision for the travel 
route. In addition, perceived qualitative team effectiveness was measured at 
the end of the experimental session by a three-item measure adopted from 
Hoegl and Gemuenden (2001; for example, “The project result was of high 
quality”) and aggregated at the group level. Cronbach’s alpha = .93.

Knowledge credibility.  On the individual level, perceived knowledge cred-
ibility was measured at the end of the experimental session with four items 
from Lewis (2003; for example, “I’m confident relying on the information 
that other team members brought to the discussion”). As in previous research 
(Ellwart  &  Konradt,  2007;  Lewis,  2003),  the  measure  demonstrated  good 
reliability. Cronbach’s alpha = .92.

Self-efficacy.  Self-efficacy was measured on the individual level with three 
items from Hertel, Konradt, and Orlikowski (2004) at the end of the experi-
mental session (e.g., “I feel capable of accomplishing the tasks within my 
team”). There have been numerous applications of this scale in past research 
that have demonstrated good reliability and validity (e.g., Ellwart & Konradt, 
2007; Geister, Konradt, & Hertel, 2006; Hertel, Deter, & Konradt, 2003).

Control variables.  We also acknowledged the potential role of other factors 
that can influence behaviors in teams such as previous acquaintance, which 
can contribute to level (rate and effectiveness) of team members’ communi-
cation. Therefore, we included acquaintance of the participants into the data 
analyses (e.g., Kozlowski & Bell, 2003). Acquaintance was measured with 
three items asking whether participants (a) have worked together in previous 
student projects, (b) have jointly participated in leisure time activities, and (c) 
are/were close friends. Cronbach’s alpha = .73.

Analyses.  For analyses on the team level, we used linear regression. To exam-
ine  whether  aggregation  was  appropriate,  intraclass  correlation  coefficient 
[ICC(2)] values were first calculated (i.e., indexes of interrater agreement; 

134 

Small Group Research 45(2)

Shrout & Fleiss, 1979). ICC(2) values relating to the perceived coordination 
(.98) and the perceived performance (.97) were adequate indicating suitable 
levels of agreement (Glick, 1985). Further support for aggregation was pro-
vided by rWG coefficients (James et al., 1984). The rWG values for perceived 
coordination and perceived team performance were .75 and .72, respectively, 
representing  satisfactory  agreement  (George,  1990;  James  et  al.,  1984). 
Finally, we calculated ICC(1) values (Shrout & Fleiss, 1979). ICC(1) values 
relating to perceived coordination ICC(1), .88, F(39, 429) = 8.33, p < .01, and 
perceived performance ICC(1), .79, F(39, 312) = 4.64, p < .01, were well 
above  the  median  level  of  ICC(1)  reported  in  the  organizational  literature 
(James et al., 1984). Together, these data provide sufficient justification for 
aggregation of the individual-level measures to the team level.

Hierarchical linear modeling (HLM) was used to predict individual-level 
outcomes such as credibility and self-efficacy (Raudenbush & Bryk, 2002). 
HLM allows a simultaneous examination of the effects of variables at both 
individual and team levels, as well as possible cross-level interaction effects 
(Raudenbush  &  Bryk,  2002).  Following  the  recommended  procedure  for 
HLM analysis (Hofmann, 1997; Raudenbush & Bryk, 2002), we first assessed 
within- and between-team variance in the dependent variable (null model). If 
significant  variance  in  individual  variables  between  teams  was  found,  an 
intercept as outcome model was estimated (Hofmann, 1997). This model esti-
mates whether the variance in the individual variables is significantly related 
to  the  TMM  of  expertise  location  (Level  2  predictor).  Acquaintance  was 
included into the model as an additional Level 2 control variable. Because the 
individuals (Level 1) are not only nested in teams (Level 2) but teams are also 
nested in the experimental condition, we modeled the experimental groups at 
Level 3 to control for the nested data structure. HLM analyses also provided 
a  multiparameter  test  for  the  variance–covariance  components  to  compare 
the deviance statistic of the null model with the intercept-as-outcome model. 
This comparison offers information when the addition of Level 2 indicators 
has a significant contribution to the explanation of variation in the outcome. 
Because the predictors were measured after completion of task one and the 
outcomes  (e.g.,  objective  expertise  ratings)  at  the  end  of  the  experiment, 
problems with common method variance were of no concern here (Podsakoff, 
MacKenzie, Lee, & Podsakoff, 2003).

Results
Means, standard deviations, and bivariate correlations are presented in Table 
1. To test whether the TMM Index is sensitive to team differences regarding 
high and low levels of meta-knowledge and consensus about the location of 

Ellwart et al. 

135

Table 2.  Means, Standard Deviations, and Within-Group Agreement of 
Experimentally Manipulated Meta-Knowledge and Consensus of Four Experimental 
Groups in Study 1.

Low meta-knowledge

High meta-knowledge

Variable

Low consensus High consensus Low consensus High consensus

Mean meta-
knowledge

Average 

deviation 
(consensus 
about meta-
knowledge)

rWG Meta-

knowledge
TMM Index 
expertise 
location

4.37 (.93)

4.05 (.73)

4.78 (.93)

5.88 (.66)

1.36 (.41)

.95 (.40)

1.23 (.48)

0.70 (.21)

.19

.61

.36

.87

3.01 (0.98)

3.09 (1.01)

3.54 (0.97)

5.18 (0.79)

expertise  (Hypothesis  1),  we  first  compared  experimental  teams  regarding 
their TMM Index of expertise location. Table 2 shows mean team score of 
expertise location, AD, and TMM Index for all experimental groups. In addi-
tion, the rWG score for the TMM items is also displayed. Results from ANOVA 
indicate that the mean team score, F(3, 36) = 9.34, p < .001, η2 = .44, AD, 
F(3, 36) = 5.71, p < .01, η2 = .32, and TMM Index, F(3, 36) = 10.11, p < .001, 
η2  =  .46,  significantly  differ  between  teams.  Moreover,  when  comparing 
teams with high consensus and low consensus, the TMM Index differs sig-
nificantly,  t(38)  =  2.17,  p  <  .05.  Similarly,  the TMM  Index  is  sensitive  to 
teams  with  high  versus  low  meta-knowledge,  t(38)  =  3.62,  p  <  .001.  As 
shown in Table 2, rWG agreement mirrors the manipulated differences regard-
ing high versus low consensus. Thus, Hypothesis 1was supported.

To validate the TMM Index (Hypotheses 2), we related the TMM Index to 
task-specific  accuracy  and  consensus  scores  following  the  procedure  from 
Austin (2003). As shown in Table 1, the TMM Index relates to the team con-
sensus score, r = .62, p < .001, as well to accuracy, r = .67, p < .001, and 
provides support for Hypotheses 2a and 2b. Thus, comparing the impersonal 
ratings  of  the  TMM  Index  with  task-  and  team-specific  naming  of  team 
experts  according  to Austin  (2003),  teams  with  high  meta-knowledge  and 
consensus  show  much  higher  agreement  and  accuracy  in  naming  domain 
experts.

136 

Small Group Research 45(2)

Table 3.  Regression Results for TMM Index and Group-Level Controls.

Study 1

Study 2

Coordination: 

Coordination: 

Step/variable

Self-ratings

Messages

Performance: 
Self-ratings

Coordination: 

Self-ratings

Performance: 
Self-ratings

Performance: 
Tutor ratings

1 Team size
1 Acquaintance
2 TMM Index
F
R2
Adjusted R2
n

.10
.21
1.25
.06
.01
40

.20

−.47**
4.99**
.21
.17
40

−.02

.39**
3.32*
.15
.11
40

.07
−.35†
.64**
4.48**
.29
.23
37

.25
−.16

.54**
3.61*
.25
.18
37

.38*
−.18
.44*
3.13*
.22
.15
37

Note. Standardized regression coefficients are reported. Significances are reported two-tailed.
†p < .10. *p < .05. **p < .01.

As predicted in Hypotheses 3a and 3b, group-level TMM Index of within-
team expertise location should predict team coordination (Hypothesis 3a) as 
well as team effectiveness (Hypothesis 3b).5 Results of hierarchical regres-
sion analyses are presented in Table 3.6 Controlled for previous acquaintance, 
team’s TMM  Index  predicts  coordination  costs,  represented  by  number  of 
received emails, β = −.47, p < .01. In teams with high meta-knowledge and 
consensus, team members show less coordination costs than teams with low 
TMM Index scores. Similarly, our data show that TMM Index predicts per-
formance self-ratings, β = .39, p < .01, but not performance time, quality, and 
perceived coordination efficacy. Thus, Hypothesis 3b received only partial 
support.

As shown in Table 4, HLM indicated that the TMM Index of the team can 
explain variance in knowledge credibility and self-efficacy on an individual 
level (Hypotheses 4a and 4b). In a first step, the null model indicates that 
there  is  significant  variance  for  knowledge  credibility  and  self-efficacy 
between  groups.  In  the  intercept-as-outcome  model,  TMM  Index  and 
acquaintance are entered into the regression. For individual knowledge cred-
ibility (Hypothesis 4a), previous acquaintance, β = .46, p < .01, and TMM 
Index of expertise location, β = .18, p < .01, explain significant amounts of 
variance for this individual variable. Deviance statistics (χ2 model compari-
son) indicate that TMM Index and acquaintance make a significant contribu-
tion  to  the  explanation  of  variation  in  the  outcome.  Thus,  controlled  by 
previous acquaintance, high meta-knowledge and consensus in a team predict 
individual trust in the knowledge and skills of fellow team members. For self-
efficacy (Hypothesis 4b), the TMM Index also explains between-group vari-
ance in the one-tailed test, β = .12, p < .05.

Ellwart et al. 

137

Table 4.  Multilevel Regression Results for TMM Index and Individual-Level 
Variables.

Step/variable

1 Team size
1 Acquaintance
2 TMM Index
ICC (null model)
ICC (Model 1)
χ2 (model comparison)

Study 1

Study 2

Knowledge 
credibility

Self-efficacy

Knowledge 
credibility

Self-efficacy

.47**
.18**
.29**
.08
5.47*

−.05

.10**
.04
.03
1.77

−.04
−.09

.28**
.16*
.02
10.2*

.04
−.15†
.26**
.12†
.01
9.4*

Note. Final estimation of Level 2 regression coefficients on Level 1 intercepts are reported. 
Significances are reported two-tailed.
†p < .10. *p < .05. **p < .01.

Discussion of Study 1
Study  1  yielded  initial  support  for  the  hypothesis  that  the TMM  Index  on 
expertise  location  is  a  sensitive  indicator  of  the  objectively  manipulated 
within-team  expertise  location  and  team  consensus.  Moreover,  the  TMM 
Index relates to the team members’ task-specific team consensus and accu-
racy scores determined from explicit participant ratings of all team members. 
On the group level, analysis yielded significant effects of the TMM Index on 
coordination variables and perceived team effectiveness, but not on team per-
formance. There are different explanations for the missing effects on perfor-
mance in this controlled laboratory setting. First, the number of subjects in 
this  experimental  setting  is  typically  lower  compared  with  field  settings 
resulting in the lack of power to detect possible effects. Moreover, from a 
methodological perspective, the samples in experiments are typically more 
homogeneous  compared  with  non-experimental  studies  which  reduce  the 
variance  in  the  groups.  Finally,  performance  in  the  experimental  situation 
was operationalized by the solution of the hidden profile. Because the task 
was artificial with little complexity compared with field tasks, participants 
were easily able to solve the task. This floor effect is probably also respon-
sible  for  the  fact  that  participants  subjectively  perceived  coordination  as 
good, thus leading to missing effects on this variable as well. However, HLM 
indicated that the TMM Index of the team explains variance for knowledge 
credibility well and marginally for self-efficacy on the individual level. This 
finding is important because it provides a valid and applicable survey mea-
surement approach of within-team expertise location.

138 

Small Group Research 45(2)

In this study, the TMM Index was experimentally manipulated to test the 
reliability and validity of the survey measure and its relationship with team 
and  individual  outcomes.  In  Study  2,  we  tested  the  generalizability  of  the 
results of Study 1 and conducted a replication study within a different context 
using a longitudinal field sample.

Study 2
The goal of Study 2 was to show that the TMM Index of expertise location 
predicts  individual-  and  group-level  outcomes  in  an  applied  field  setting. 
Although Study 2 is correlational in nature, separate times for the assessment 
of predictor and criteria variables reduces biases of common method variance 
(Podsakoff et al., 2003) and allows for better interpretations compared with 
single-point cross-sectional designs.

Method
Sample.  Participants  in  Study  2  were  130  students  (105  females  and  25 
males; M age = 24 years, SD = 5.0) recruited from a large university in Ger-
many. Organized into 37 teams, they worked together in an undergraduate 
course over a period of 3 months to gain course credit. All teams worked on 
a small-scale research project on developing, conducting, and evaluating a 
scientific research question assisted by a course tutor. The areas of expertise 
were literature search, test planning, data collection, statistical analyses, as 
well as oral and written presentation of the results. Team size ranged from 
three to five members (M = 3.4, SD = 0.8).

Procedure.  Student teams were assessed in an early (T1) and later (T2) phase 
of the project. Following a typology by King and Cleland (1988), the teams 
worked in the T1 conceptual and definition phase (about 2 weeks after team 
start), evaluating objectives and strategies and determining schedule, perfor-
mance, and resource requirements. At T2 (about 10 weeks after team start), 
teams were in their operational phase, collecting data or working on analyses 
and presentations.

Measures.  To ensure comparability, we used the same measures as in Study 
1. For Study 2, Cronbach’s alpha was .92 for TMM Index, .84 for coordina-
tion, .91 for knowledge credibility, .83 for self-efficacy, and .81 for aggre-
gated  team  performance.  TMM  Index  as  well  as  accuracy  and  consensus 
ratings were measured at T1. Knowledge credibility, self-efficacy, perceived 

Ellwart et al. 

139

coordination  success  as  well  as  performance  (self-  and  tutor  ratings)  were 
measured at T2.

Task-specific  consensus  and  accuracy  ratings.  For  validation  of  the TMM 
Index, consensus and accuracy scores were again calculated based on team 
members’  explicit  identification  of  within-team  experts  and  their  expertise 
domain self-ratings. Prior interviews with the team instructors revealed six 
domains of expertise closely related to the students’ projects: coordination/
organization, theory/literature, task planning, statistics, written reports, and 
oral  presentation.  Objective  consensus  was  calculated  by  measuring  team 
consensus on who was the team expert for each of the six domains. Teams 
whose members identified the same individual as an expert on a given domain 
were considered to have high task consensus. The calculation was identical to 
Study 1 (cf. Austin, 2003).

Task-specific accuracy was measured by combining self- and team mem-
bers’  identifications  of  expertise.  Accuracy  was  determined  by  matching 
team member identification of experts with self-report ratings of expertise. 
The more the identified experts concurred with the team member (i.e., they 
rated themselves as experts in that domain), the higher the team accuracy rat-
ing was. Following Austin (2003), the accuracy measure was calculated by 
first determining separate scores of expertise accuracy for each team mem-
ber. Expertise accuracy was determined by matching team member identifi-
cation  of  expertise  with  self-report  ratings  of  member  expertise.  This 
individual-level perceived expertise accuracy score was averaged to the team 
level to create the team accuracy score (for details on the computation, see 
Austin,  2003).  High  team  accuracy  indicates  that  team  members  correctly 
identified the experts who considered themselves as experts in a particular 
area.

Team  performance.  In  addition  to  performance  self-ratings  aggregated 
at the team level, team performance was also evaluated by the tutor. Each 
instructor  rated  how  the  team  performed  using  the  measure  by  Hoegl  and 
Gemuenden (2001). Cronbach’s alpha = .83.

Control variables.  Acquaintance (M = .98, SD = 0.58) and team size (M = 
3.32, SD = 0.98) may contribute to the level of an outcome (e.g., Kozlowski 
& Bell, 2003) and were used as controls. Acquaintance was measured with 
the same three items used in Study 1.

Analyses.  Similar to Study 1, we applied linear regression for team-level pre-
diction. To examine whether aggregation was appropriate, first ICC(2) values 

140 

Small Group Research 45(2)

were calculated (i.e., indexes of interrater agreement; Shrout & Fleiss, 1979). 
ICC(2) values relating to the perceived coordination (.94) and the perceived 
performance  (.95)  were  adequate,  indicating  suitable  levels  of  agreement 
(Glick, 1985). Further support for aggregation was provided by rWG coeffi-
cients (James et al., 1984). The rWG values for perceived coordination and 
perceived team performance were .74 and .76, respectively, representing sat-
isfactory agreement (George, 1990; James et al., 1984). Finally, we calcu-
lated  ICC(1)  values  (Shrout  &  Fleiss,  1979).  ICC(1)  values  relating  to 
perceived coordination ICC(1), .88, F(36, 396) = 8.02, p < .01, and perceived 
performance ICC(1), .85, F(36, 288) = 6.87, p < .01, were well above the 
median level of ICC(1) reported in the organizational literature (James et al., 
1984). Together, these data provide ample justification for aggregation of the 
individual-level measures to the team level. HLM was applied for predicting 
credibility and self-efficacy.

Results
Means, standard deviations, and bivariate correlations are presented in Table 
1. Descriptive statistics indicate that teams differ with regard to meta-knowl-
edge and consensus. Reported meta-knowledge ranges from 1.38 to 4.67 (M 
= 3.46, SD = 0.78). Most interesting, AD of the reported meta-knowledge 
ranged from .13 to .88 (M = 0.48, SD = 0.19). Transferred to rWG scores, inter-
rater  agreement  regarding  meta-knowledge  is  .74  (SD  =  0.16)  across  all 
teams, with 13 teams (35%) below the threshold of .70. This indicates that 
consensus about the expertise location varies between teams as a phenome-
non of TMM.

To validate the TMM Index in the field sample (Hypotheses 2a and 2b), 
we related the TMM Index on T1 to task-specific accuracy and consensus 
scores  (T1)  following Austin  (2003).  These  scores  are  based  on  self-  and 
other ratings of all team members on six specific domains of expertise. As 
suggested in Hypothesis 2a, the TMM Index relates to the team consensus 
score, r = .50, p < .01, as well as to accuracy as suggested in Hypothesis 2b, 
r = .45, p < .01. Table 1 displays the results. Thus, teams with TMM indices 
of high meta-knowledge and consensus show more agreement and accuracy 
in naming the correct experts for different task domains.

Hypotheses 3a and 3b, predicting that group-level TMM Index of exper-
tise location on T1 should predict team’s coordination success (Hypothesis 
3a) as well as team’s effectiveness (Hypothesis 3b) in the later phase of the 
project (T2), were supported. Results from hierarchical regression analyses 
are presented in Table 3. Controlled for previous acquaintance and team size, 
team’s TMM Index predicts perceived coordination, β = .64, p < .01, as well 

Ellwart et al. 

141

as team performance perceived by team members, β = .54, p < .01, and the 
tutor, β = .44, p < .01. Teams with high TMM indices of expertise location in 
the beginning of the project (T1) report better coordination and performance 
during the operational phase assessed at T2.

To test Hypotheses 4, individual-level outcomes on T2 were analyzed by 
HLM.  Intraclass  correlations  (ICC)  indicate  significant  variance  between 
teams regarding knowledge credibility and self-efficacy. In the intercept-as-
outcome model, acquaintance, team size, and TMM Index are entered into 
the  regression  (see  Table  4).  Predicting  knowledge  credibility,  the  TMM 
Index of expertise location, β = .28, p < .01, can explain variance for this 
individual  variable  (Hypothesis  4a).  Controlled  for  acquaintance  and  team 
size, high meta-knowledge and consensus in a team predicts individual trust 
in  the  knowledge  and  skills  of  the  team  members.  For  self-efficacy 
(Hypothesis 4b), the TMM Index significantly explains group variance, β = 
.26, p < .01. Thus, in teams with high meta-knowledge and consensus about 
expertise  location,  team  members  feel  capable  of  accomplishing  the  team 
tasks. Deviance statistics for both regressions indicate that the TMM Index 
contributes significantly to the explanation of variation in the outcome.

Discussion of Study 2
Study 2 reassessed and replicated the findings from Study 1 in a longitudinal 
field setting using student project teams. Results showed that the TMM Index 
again was related to task-specific consensus and accuracy scores from trans-
active memory research (Austin, 2003). Thus, the TMM Index represents an 
effective and short method to screen teams’ meta-knowledge and consensus. 
Moreover,  TMM  indices  assessed  3  weeks  after  team  start  predicted  per-
ceived  team  coordination  and  performance,  both  rated  by  the  responsible 
tutor and the team members, respectively. Finally, results from HLM showed 
that the TMM Index can explain knowledge credibility and self-efficacy on 
the individual level.

General Discussion
The aim of this article was to provide and validate a field screening measure 
that integrates (a) the quality of knowledge about the experts within the team 
(meta-knowledge)  and  (b)  the  team’s  consensus  on  within-team  expertise 
into one TMM Index. First and foremost, this measure should be applicable 
across different types of teams and tasks in organizational surveys and should 
confirm  relationships  with  outcome  variables  such  as  teams’  effectiveness 
and attitudes. Results from our experimental study (Study 1) and field study 

142 

Small Group Research 45(2)

(Study  2)  clearly  demonstrate  that  the  TMM  Index  is  a  valid  measure  of 
expertise location in teams. In the experimental study, meta-knowledge about 
expertise  location  and  consensus  was  objectively  manipulated  within  each 
team. The TMM Index was able to differentiate between these teams. This 
gives support that both the dimensions of meta-knowledge and consensus are 
two relevant aspects of TMM that can be represented in one index. In both 
studies, the TMM Index was able to predict relevant variables on the team 
and individual level. As suggested, the TMM Index also predicted team coor-
dination  success  (Lewis,  2003)  and  team  performance  (Mohammed  et  al., 
2010). Using multilevel analyses, the TMM Index predicted individual trust 
in the knowledge of the teammates (knowledge credibility; Lewis, 2003) and 
self-efficacy (Peterson et al., 2000). The empirical data support the assump-
tion that the TMM Index is a conceptually and statistically valid measure of 
meta-knowledge and consensus about the location of expertise within teams. 
However, the task-independent characteristics of the measure make it appro-
priate for field settings and allow comparisons between different teams and 
tasks  in  organizational  change  settings  or  action  research  (Burke,  2011). 
Convergent and criterion-related validity was shown through its relationship 
to established task-specific measures of accuracy and consensus introduced 
by Austin (2003).

From a methodological perspective, Study 1 was an experimental manipu-
lation of meta-knowledge and consensus, whereas Study 2 focused on field 
data.  Most  importantly,  the  experimental  manipulation  in  Study  1  allowed 
controlling the objective meta-knowledge and consensus about the location 
of within-team expertise to verify the sensitivity of the index. Study 2 was 
designed to replicate these findings in a field setting. The advantage of testing 
the  TMM  Index  in  both  a  field  application  and  in  a  controlled  laboratory 
experiment is that the strengths of one method can, to some extent, compen-
sate for the weaknesses of the other. The strengths of Study 1 include the fact 
that the structure of team knowledge of within-team expertise location was 
controlled and a link toward outcomes was supported. The added value of 
Study 2 is that the replication of these outcomes in multiple team settings 
demonstrates  generalizability.  An  obvious  weakness  is  its  correlational 
nature. However, by measuring predictors and criteria at different times, the 
overestimation of the relationships between variables and influence of com-
mon method biases could be reduced in both studies (Podsakoff et al., 2003). 
Moreover,  specific  analyses  of  participants’  task-specific  ratings  on  team 
experts, identification of their own expertise, and consequent accuracy rat-
ings indicated construct validity. A second methodological advantage can be 
found in the application of multilevel analyses. Variables such as knowledge 
credibility and self-efficacy are outcomes on the individual level, whereas the 

Ellwart et al. 

143

shared mental model index of expertise location is a group-level variable. To 
comply with the nested data, HLM was used to predict individual-level vari-
ables (Raudenbush & Bryk, 2002). In sum, the fact that the validity of the 
TMM Index of within-team expertise location was supported across method-
ologies  provides  valuable  support  for  the  argument  that  this  measure  is  a 
useful screening approach for assessing expertise location in teams.

Value of the TMM Index of Expertise Location for 
Organizational Surveys
A major aim and advantage of the TMM Index is its task- and team-indepen-
dent practicability, for example, in field settings as a first indicator or screen-
ing  about  the  quality  and  the  consensus  of  the  teams’  knowledge  about 
expertise location. Although various measures of team knowledge have been 
introduced  in  recent  years  (cf.  Cooke  et  al.,  2000;  DeChurch  &  Mesmer-
Magnus, 2010b; Langan-Fox et al., 2000; Mohammed et al., 2010), there is a 
clear lack of measures that are suitable for application in the field (Lewis, 
2003; Rentsch & Mot, 2012).

In the present contribution, we defined three characteristics of the TMM 
Index: First, the measure does not depend on a specific team or task and can 
be  applied  across  settings,  independent  of  the  specific  areas  of  expertise 
within the teams surveyed. Second, the measure requires a low investment of 
resources (time and effort) for administration. Third, the TMM measure cap-
tures the quality of knowledge about expertise location as well as the consen-
sus about it on a team level.

Existing  methods  from  research  on  TMM  (e.g.,  pathfinder  networks, 
UCINET, concept mapping) are excellent tools for research in a controlled 
and well-defined research setting to measure the underlying structure of team 
cognition. However, applications with regard to expertise location would rely 
on a clear a priori definition of all possible areas the team has expertise to 
handle and of the expertise domains of all team members. During measure-
ment, participants would have to rate a priori defined expertise domains ver-
sus concepts in an elaborate way. Such approaches are not practicable for the 
settings  of  early  team  screenings  because  of  the  effort  and  complexity  to 
determine  domains  of  expertise  across  different  teams  or  organizations. 
Moreover, it is difficult to convince field partners to apply such time-con-
suming measurement approaches, especially in the early phases of projects. 
Perceptual measures of TMM based on agreement indices derived from ques-
tionnaires consisting of Likert-type scales are suitable (e.g., Eby et al., 1999; 
Levesque  et  al.,  2001; Webber  et  al.,  2000)  but  only  offer  an  indicator  of 

144 

Small Group Research 45(2)

consensus. However, this indicator does not reveal whether the level of agree-
ment is associated with knowing versus not knowing the experts. With the 
TMM Index of within-team expertise location, both the quality of knowledge 
about  expertise  location  and  team  consensus  are  integrated  into  a  single 
index. For this reason, the TMM Index of expertise location offers an eco-
nomic screening measurement approach that can be used across various team 
and task types. Although the TMM Index cannot capture the specific underly-
ing organizational structure of the specific knowledge domains, it is clearly 
related  to  objective  team-  and  task-specific  ratings  and  can  be  used  as  a 
screening tool prior to more extensive investigations of specific teams. Thus, 
the TMM Index is a valuable alternative for many field applications in orga-
nizations  where  laborious  and  complex  methods  to  assess  knowledge  and 
consensus are difficult to apply. The application may be practically oriented, 
for example, in organizational change settings (Burke, 2011), or research ori-
ented in generating and explaining theoretical models of TMM. For example, 
present research on TMM explores the mechanisms of team learning or lead-
ership and discusses TMM as a cognitive mediator in explaining team perfor-
mance  (Wiedow,  Konradt,  Ellwart,  &  Steenfatt,  in  press;  DeChurch  & 
Mesmer-Magnus, 2010a).

Limitations
There are some limitations to the methodological aspects of these two studies 
as well as some shortcomings of the TMM Index. The first limitation is that 
both  studies  were  conducted  with  student  participants,  which  casts  some 
measure of doubt on the robustness of the results. However, the second study 
was done with students who participated in actual teams that were assigned 
objectives  to  accomplish  to  gain  course  credits.  Moreover,  Ellwart  et  al. 
(2014)  applied  the  same  version  of  a  within-team  expertise  location  scale 
from  Faraj  and  Sproull  (2000)  and  found  similar  effects  on  knowledge 
exchange and identification. They allowed a reanalysis of their data by com-
puting the TMM Index following the procedure applied in this study. Their 
field data from 73 organizational teams reveal correlations of the TMM Index 
on expertise location (data from team members) with supervisor ratings of 
team knowledge exchange (r = .21, p < .07; three-item scale by Ellwart & 
Konradt, 2007) and innovation behavior (r = .24, p < .04; three-item scale by 
Jannsen,  2000).7 Although  these  results  suggest  the  potential  of  the TMM 
Index in organizational settings, future research should examine its validity 
across different in situ jobs and organizations.

Second,  notwithstanding  the  advantages  of  the  TMM  Index  of  within-
team expertise location to integrate knowledge and consensus into a single 

Ellwart et al. 

145

measure, the size of the index is represented by a linear transformation of the 
difference between meta-knowledge and the consensus. High scores indicate 
high  agreement  on  existing  meta-knowledge,  whereas  low  scores  indicate 
less meta-knowledge and agreement. Although results in Study 1 indicate the 
sensitivity of the index to both dimensions, research might be useful regard-
ing the separate effects and interactions of both indicators. Similar to a study 
by Bliese and Halverson (1998) on team consensus and psychological well-
being, one could conceive of a regression approach entering the main effects 
of meta-knowledge, consensus, and finally, the interaction term. This would 
offer  researchers  valuable  information  on  the  incremental  impact  of  each 
dimension on team or individual variables. However, there are some statisti-
cal and practical shortcomings. From a statistical point of view, consensus 
and meta-knowledge might be highly correlated in field situations, leading to 
multicollinearity of the predictors, thus forcing the researcher to combine the 
variable. From an applied perspective, practitioners apply the TMM Index to 
determine where knowledge about within-team expertise is lacking in orga-
nization  teams.  If  the  index  indicates  teams  with  relative  deficits  in  the 
knowledge about team experts, further assessments and team development 
programs can follow. In the present studies, it was of less interest to illustrate 
what main effect or interaction could explain a model, as the present impera-
tive was to demonstrate the index as a valid predictor of team performance. 
Given that this was shown in both studies, the single index has been validated 
as a practicable and valuable tool.

Third, the linear transformation between knowledge and consensus (AD) 
offers further lines of research. In the present state, meta-knowledge and AD 
are weighted equally in the same metric (Burke & Dunlap, 2002), assuming 
that AD and absolute meta-knowledge have a similar impact on the TMM. It 
would be of conceptual and practical interest to investigate whether a higher 
impact of consensus or meta-knowledge leads to better predictions of perfor-
mance. For example, if consensus is the crucial variable for some team tasks, 
one could enlarge the impact of the AD by enhancing the impact of AD on the 
TMM Index. Further research may be worthwhile to examine the effects of 
different weights on performance indices in simulation studies to optimize 
the predictive validity of the TMM Index.

A final point addresses the limitation of the TMM Index compared with 
the structural measures applied in TMM research. This perceptual measure 
operationalizes the content of cognition but not the degree of structural simi-
larity  of  the  mental  representation  (Rentsch  &  Mot,  2012).  Because  of  its 
disregard of the structural dimension, Mohammed et al. (2010) would argue 
that this Likert-type–based approach is not a TMM measurement technique 
in  its  own  right,  whereas  others  would  classify  the  approach  as  a  TMM 

146 

Small Group Research 45(2)

elicitation  method  compared  with  the  structural  assessment  of  TMM 
(DeChurch & Mesmer-Magnus, 2010b). In sum, both the perceptual and the 
structural approach tap into the content of TMMs (Rentsch & Mot, 2012), but 
in  doing  so,  they  draw  complementary  pictures  of  team  cognition.  In  this 
vein, the perceptional approach of the TMM Index developed in this article 
does not rule out the structural measures of TMM for application and research 
but rather serves as a complement to such measures because it is applicable 
for specific context situations and samples.

Conclusion
We conclude that the TMM Index is useful to screen TMM about expertise 
location in field settings and across a variety of team types and tasks. The 
measure, which integrates consensus scores and means ratings into a single 
predictive index, could—due to its brevity—easily be applied in other con-
tent domains of TMM research and may be a lucrative topic of further inves-
tigations. For example, task-relevant knowledge could be measured by asking 
team members whether they know about the goals and/or strategies of the 
team. As a first screening indicator, the TMM Index gives researchers and 
practitioners helpful initial insights into team cognition but requires deeper 
and more elaborated subsequent analyses to get a complete picture of team 
mental representations.

Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interests with respect to the research, 
authorship and/or publication of this article.

Funding
The author(s) received no financial support for the research, authorship, and/or publi-
cation of this article.

Notes
1. 

In the literature, there are several terms referring to team mental models (TMMs) 
such as teamwork mental models, shared mental model, shared cognition, team 
cognition,  team  member  schema  similarity.  Related  to  the  concept  of  TMM 
of  expertise  location  is  the  conceptualization  of  transactive  memory  systems 
(Hollingshead,  Gupta,  Yoon,  &  Brandon,  2012’  Wegner,  1986,  1995).There 
are two similarities between transactive memory systems and TMM of exper-
tise location: First, the content domain of knowledge about expertise within the 
team and second, the sharedness of this knowledge in terms of the agreement. 
However, transactive memory systems are not TMM; rather they go further, as 

Ellwart et al. 

147

they explain “how people in collectives learn, store, use, and coordinate their 
knowledge”  (Hollingshead  et  al.,  2012,  p.  421).  In  this  regard,  Mohammed, 
Ferzandi, and Hamilton (2010) classify transactive memory systems as taskwork 
knowledge, whereas TMM of expertise location focus on the skills and knowl-
edge of the team members (teamwork TMM). For a comprehensive review, see 
also DeChurch and Mesmer-Magnus (2010b). Besides the group-level phenom-
enon of TMM of expertise location, the concept of expertise recognition is also 
discussed  as  a  characteristic  of  individual  team  members.  There  is  evidence 
from experimental social psychological as well as from applied field research 
that individual recognition of fellow team members’ expertise is important for 
group  decision  making  and  team  performance  (Ellwart,  Buendgens,  &  Rack, 
2014; Faraj & Sproull, 2000; Littlepage & Mueller, 1997; Stasser, Stewart, & 
Wittenbaum, 1995).

2.  Perceptual measures operationalize the content of cognition but not the degree 
of  structural  similarity  of  the  mental  representation  (Rentsch  &  Mot,  2012). 
Because  of  neglect  of  the  structural  dimension,  some  authors  argue  that  the 
Likert-type–based approaches do not constitute a TMM measurement technique 
in its own right (Mohammed et al., 2010), whereas others would classify percep-
tual  approaches  as  TMM  elicitation  methods  (DeChurch  &  Mesmer-Magnus, 
2010b). For the goal of initial screening, the approach is valuable because the 
Likert-type–based  approach  allows  developing  a  task-  and  team-independent 
measure. However, this type of measure only represents a first elicitation of the 
TMM  of  organizational  groups;  for  examining  the  structure  and  accuracy  in-
depth, detailed structural methods are necessary.

3.  The  adopted  items  were  tested  and  validated  in  a  previous  study  by  Ellwart 
and Konradt (2007). They were translated from the original scale by Faraj and 
Sproull (2000), following the forward–backward procedure proposed by Brislin 
(1980) to arrive at conceptual equivalence.

4.  For  estimating  consensus  in  the  proposed TMM  Index,  the  average  deviation 
(AD) has two major advantages. First, AD indices do not require the determi-
nation of a null random response distribution as for rWG, which is the primary 
determinant of the quality of the agreement estimate. Second, AD is computed 
relative to the mean of an item and thus provides direct conceptualizations in the 
same metric of the original measure (Burke & Dunlap, 2002). The same metric 
allows relating the team consensus scores on expertise location (AD) to the team 
members’ mean level of knowledge of expertise location in one single coeffi-
cient, which we denote as the TMM Index.

5.  A check for distributional assumptions of maximum likelihood estimation reveals 
that univariate and multivariate skewness and kurtosis were within acceptable 
range (cf. Tabachnick & Fidell, 2001).

6.  Although the validation focuses on the regression between the TMM Index and 
the outcomes, the teams analyzed in the regression are nested in experimental 
groups. To respect the nested structure of the data, we performed hierarchical 
linear modeling (HLM) analyses between TMM Index and team outcomes by 

148 

Small Group Research 45(2)

modeling the experimental groups as upper level variable. The results of HLM 
are comparable with simple regressions displayed in Table 3. Additional HLM 
analyses are available on request. We wish to thank the reviewers for the advice 
concerning the nested structure.

7.  We appreciate the permission granted to apply the TMM Index to these data.

References
Austin,  J.  R.  (2003).  Transactive  memory  in  organizational  groups:  The  effects  of 
content, consensus, specialization, and accuracy on group performance. Journal 
of Applied Psychology, 88, 866-878. doi:10.1037/0021-9010.88.5.866

Bandura, A. (1977). Self-efficacy: Toward a unifying theory of behavioral change. 

Psychological Review, 84, 191-215. doi:10.1037/0033-295X.84.2.191

Bandura, A. (1997). Self-efficacy: The exercise of control. New York, NY: Freeman.
Bliese, P. D., & Halverson, R. R. (1998). Group consensus and psychological well-
being: A large field study. Journal of Applied Social Psychology, 28, 563-580. 
doi:10.1111/j.1559-1816.1998.tb01720.x

Brislin, R. W. (1980). Translation and content analysis of oral and written materials. 
In H. C. Triandis & W. Lonner (Eds.), Handbook of cross-cultural psychology: 
Methodology (pp. 389-444). Boston, MA: Allyn & Bacon.

Brown, R. D., & Hauenstein, N. M. A. (2005). Interrater agreement reconsidered: An 
alternative  to  the  rWG  indices.  Organizational  Research  Methods,  8,  165-184. 
doi:10.1177/1094428105275376

Burke, M. J., & Dunlap, W. P. (2002). Estimating interrater agreement with the aver-
age deviation index: A user’s guide. Organizational Research Methods, 5, 159-
172. doi:10.1177/1094428102005002002

Burke, M. J., Finkelstein, L. M., & Dusig, M. S. (1999). On average deviation indices 
for estimating interrater agreement. Organizational Research Methods, 2, 49-68. 
doi:10.1177/109442819921004

Burke, W. W. (2011). Organization change: Theory and practice (3rd ed.). Thousand 

Oaks, CA: SAGE.

Cannon-Bowers, J. A., Salas, E., & Converse, S. (1993). Shared mental models in 
expert team decision making. In N. J. Castellan Jr. (Ed.), Individual and group 
decision making: Current issues (pp. 221-246). Hillsdale, NJ: Lawrence Erlbaum.
Chin,  R.,  Benne,  K.  D.,  &  Bennis,  W.  G.  (1985).  General  strategies  for  effecting 

changes in human systems. New York, NY: Holt, Rinehart, and Winston.

Chou, L., Wang, A., Wang, T., Huang, M., & Cheng, B. (2008). Shared work values 
and team member effectiveness: The mediation of trustfulness and trustworthi-
ness. Human Relations, 61, 1713-1742. doi:10.1177/0018726708098083

Cooke, N. J., Salas, E., Cannon-Bowers, J. A., & Stout, R. J. (2000). Measuring team 

knowledge. Human Factors, 42, 151-173. doi:10.1518/001872000779656561

DeChurch, L. A., & Mesmer-Magnus, J. R. (2010a). The cognitive underpinnings of 
effective teamwork: A meta-analysis. Journal of Applied Psychology, 95, 32-53. 
doi:10.1037/a0017328

Ellwart et al. 

149

DeChurch, L. A., & Mesmer-Magnus, J. R. (2010b). Measuring shared team mental 
models: A meta-analysis. Group Dynamics: Theory, Research, and Practice, 14, 
1-14. doi:10.1037/a0017455

Eby,  L.  T.,  Meade,  A.  W.,  Parisi,  A.  G.,  &  Douthitt,  S.  (1999).  The  development 
of an individual-level teamwork expectations measure and the application of a 
within-group  agreement  statistic  to  assess  shared  expectations  for  teamwork. 
Organizational Research Methods, 2, 366-394. doi:10.1177/109442819924003

Ellwart,  T.,  Biemann,  T.,  &  Rack,  O.  (2011).  Measurement  of  team  knowledge  in 
the field—Methodological advantages and limitations. In M. Boos, M. Kolbe, P. 
Kappeler, & T. Ellwart (Eds.), Coordination in human and primate groups (pp. 
155-176). Heidelberg, Germany: Springer-Verlag.

Ellwart, T., Buendgens, S., & Rack, O. (2014). Managing knowledge exchange and 
identification in age diverse teams. Journal of Managerial Psychology, 28, 950-
972. doi 10.1108/JMP-06-2013-0181

Ellwart,  T.,  &  Konradt,  U.  (2007).  Wissensverteilung  und  Wissenskoordination 
in  Gruppen:  Überprüfung  deutschsprachiger  Skalen  unter  computergestütz-
ter  Gruppenarbeit  [Knowledge  distribution  and  knowledge  coordination  in 
groups:  An  examination  of  German-language  scales  in  computer-mediated 
teamwork]. Zeitschrift für Arbeits- und Organisationspsychologie, 51, 128-135. 
doi:10.1026/0932-4089.51.3.128

Espinosa, J. A., Lerch, J., & Kraut, R. (2004). Explicit vs. implicit coordination mech-
anisms and task dependencies: One size does not fit all. In E. Salas & S. M. Fiore 
(Eds.), Team cognition: Understanding the factors that drive process and perfor-
mance (pp. 107-129). Washington, DC: APA Books.

Faraj, S., & Sproull, L. (2000). Coordinating expertise in software development teams. 

Management Science, 46, 1554-1568. doi:10.1287/mnsc.46.12.1554.12072

Geister, S., Konradt, U., & Hertel, G. (2006). Effects of process feedback on motiva-
tion, satisfaction, and performance in virtual teams. Small Group Research, 37, 
459-489. doi:10.1177/1046496406292337

George, J. M. (1990). Personality, affect, and behavior in groups. Journal of Applied 

Psychology, 75, 107-116. doi:10.1037//0021-9010.75.2.107

Glick, W. H. (1985). Conceptualizing and measuring organizational and psychologi-
cal climate: Pitfalls in multilevel research. Academy of Management Journal, 10, 
601-616. doi:10.5465/AMR.1985.4279045

Hertel,  G.,  Deter,  C.,  &  Konradt,  U.  (2003).  Motivation  gains  in  computer-medi-
ated  work  groups.  Journal  of  Applied  Social  Psychology,  33,  2080-2105. 
doi:10.1111/j.1559-1816.2003.tb01876.x

Hertel,  G.,  Konradt,  U.,  &  Orlikowski,  B.  (2004).  Managing  distance  by  interde-
pendence:  Goal  setting,  task  interdependence,  and  team-based  rewards  in  vir-
tual teams. European Journal of Work & Organizational Psychology, 13, 1-28. 
doi:10.1080/13594320344000228

Hoegl, M., & Gemuenden, H. G. (2001). Teamwork quality and success of innovative 

projects. Organization Science, 12, 435-449. doi:10.1287/orsc.12.4.435.10635

Hofmann, D. A. (1997). An overview of the logic and rationale of hierarchical linear 
models. Journal of Management, 23, 723-744. doi:10.1177/014920639702300602

150 

Small Group Research 45(2)

Hollingshead, A. B. (1998). Distributed knowledge and transactive processes in deci-
sion-making groups. In M. A. Neal, E. A. Mannix, & D. H. Gruenfeld (Eds.), 
Research on managing groups and teams (pp. 103-123). Stanford, CA: JAI Press.
Hollingshead,  A.  B.,  Gupta,  N.,  Yoon,  K.,  &  Brandon,  D.  P.  (2012).  Transactive 
memory theory and teams: Past, present and future. In E. Salas, S. M. Fiore, & 
M. P. Letsky (Eds.), Theories of team cognition: Cross-disciplinary perspectives 
(pp. 421-455). New York, NY: Routledge.

James, L. R., Demaree, R. G., & Wolf, G. (1984). Estimating within-group interrater 
reliability with and without response bias. Journal of Applied Psychology, 69, 
85-98. doi:10.1037//0021-9010.69.1.85

Jannsen, O. (2000). Job demands, perceptions of reward fairness and innovative work 
behaviour.  Journal  of  Occupational  and  Organizational  Psychology,  73,  287-
302. doi:10.1348/096317900167038

King, W. R., & Cleland, D. I. (1988). Life-cycle management. In W. R. King & D. 
I. Cleland (Eds.), Project management handbook (2nd ed., pp. 191-230). New 
York, NY: van Nostrand Reinhold.

Klein, K. J., Conn, A. B., Smith, D. B., & Sorra, J. S. (2001). Is everyone in agree-
ment? An exploration of within-group agreement in employee perceptions of the 
work environment. Journal of Applied Psychology, 86, 3-16. doi:10.1037/0021-
9010.86.1.3

Klimoski, R. J., & Mohammed, S. (1994). Team mental model: Construct or meta-
phor? Journal of Management, 20, 403-437. doi:10.1177/014920639402000206
Kozlowski, S. W. J., & Bell, B. S. (2003). Work groups and teams in organizations. 
In W. C. Borman, D. R. Ilgen, & R. J. Klimoski (Eds.), Handbook of psychol-
ogy: Vol. 12. Industrial and organizational psychology (pp. 333-375). London, 
England: Wiley.

Kozlowski, S. W. J., & Chao, G. T. (2012). The dynamics of emergence: Cognition 
and cohesion in work teams. Managerial and Decision Economics, 33, 335-354. 
doi:10.1002/mde.2552

Kozlowski,  S.  W.  J.,  &  Ilgen,  D.  R.  (2006).  Enhancing  the  effectiveness  of  work 
groups  and  teams.  Psychological  Science  in  the  Public  Interest,  7,  77-124. 
doi:10.1111/j.1529-1006.2006.00030.x

Langan-Fox,  J.,  Code,  S.,  &  Langfield-Smith,  K.  (2000).  Team  mental  models: 
Techniques,  methods  and  analytic  approaches.  Human  Factors,  42,  242-271. 
doi:10.1518/001872000779656534

Levesque,  L.  L.,  Wilson,  J.  M.,  &  Wholey,  D.  R.  (2001).  Cognitive  divergence 
and  shared  mental  models  in  software  development  project  teams.  Journal  of 
Organizational Behavior, 22, 135-144. doi:10.1002/job.87

Lewis,  K.  (2003).  Measuring  transactive  memory  systems  in  the  field:  Scale 
development  and  validation.  Journal  of  Applied  Psychology,  88,  587-604. 
doi:10.1037/0021-9010.88.4.587

Littlepage, G. E., & Mueller, A. (1997). Recognition and utilization of expertise in 
problem-solving groups: Expert characteristics and behavior. Group Dynamics, 
1, 324-328. doi:10.1037/1089-2699.1.4.324

Ellwart et al. 

151

Marks, M. A., Sabella, M. J., Burke, C. S., & Zaccaro, S. J. (2002). The impact of 
cross-training on team effectiveness. Journal of Applied Psychology, 87, 3-13. 
doi:10.1037//0021-9010.87.1.3

Marks,  M.  A.,  Zaccaro,  S.  J.,  &  Mathieu,  J.  E.  (2000).  Performance  implications 
of  leader  briefings  and  team-interaction  training  for  team  adaptation  to  novel 
environments. Journal of Applied Psychology, 85, 971-986. doi:10.1037/0021-
9010.85.6.971

Mathieu, J. E., Heffner, T. S., Goodwin, G. F., Cannon-Bowers, J. A., & Salas, E. 
(2005). Scaling the quality of teammates’ mental models: Equifinality and norma-
tive comparisons. Journal of Organizational Behavior, 26, 37-56. doi:10.1002/
job.296

Mathieu, J. E., Heffner, T. S., Goodwin, G. F., Salas, E., & Cannon-Bowers, J. A. 
(2000). The influence of shared mental models on team process and performance. 
Journal of Applied Psychology, 85, 273-283. doi:10.1037//0021-9010.85.2.273

Mohammed, S., Ferzandi, L., & Hamilton, K. (2010). Metaphor no more: A 15-year 
review of the team mental model construct. Journal of Management, 36, 876-
910. doi:10.1177/0149206309356804

Mohammed, S., Klimoski, R. J., & Rentsch, J. R. (2000). The measurement of team 
mental models: We have no shared schema. Organizational Research Methods, 
3, 123-165. doi:10.1177/109442810032001

Mohammed,  S.,  Tesler,  R.,  &  Hamilton,  K.  (2012).  Time  and  shared  cognition: 
Towards greater integration of temporal dynamics. In E. Salas, S. Fiore, & M. P. 
Letsky (Eds.), Theories of team cognition: Cross disciplinary perspectives (pp. 
87-116). New York, NY: Taylor and Francis.

Moreland,  R.  L.,  &  Myaskovsky,  L.  (2000).  Exploring  the  performance  ben-
efits  of  group  training:  Transactive  memory  or  improved  communication? 
Organizational  Behavior  and  Human  Decision  Processes,  82,  117-133. 
doi:10.1006/obhd.2000.2891

Peterson,  E.,  Mitchell,  T.,  Thompson,  L.,  &  Burr,  R.  (2000).  Collective  effi-
cacy  and  aspects  of  shared  mental  models  as  predictors  of  performance  over 
time  in  work  groups.  Group  Processes  &  Intergroup  Relations,  3,  296-316. 
doi:10.1177/1368430200033005

Podsakoff,  P.  M.,  MacKenzie,  S.  B.,  Lee,  J.-Y.,  &  Podsakoff,  N.  P.  (2003). 
Common method biases in behavioral research: A critical review of the litera-
ture and recommended remedies. Journal of Applied Psychology, 88, 879-903. 
doi:10.1037/0021-9010.88.5.879

Raudenbush,  S.  W.,  &  Bryk,  A.  S.  (2002).  Hierarchical  linear  models  (2nd  ed.). 

Thousand Oaks, CA: SAGE.

Rentsch, J. R., & Mot, I. R. (2012). Elaborating cognition in teams: Cognitive similar-
ity configurations. In E. Salas, S. M. Fiore, & M. P. Letsky (Eds.), Theories of 
team cognition: Cross-disciplinary perspectives (pp. 145-170). New York, NY: 
Routledge.

Rentsch, J. R., Small, E. E., & Hanges, P. J. (2008). Cognitions in organizations and 
teams: What is the meaning of cognitive similarity? In B. Smith (Ed.), The people 

152 

Small Group Research 45(2)

make the place: Exploring dynamic linkages between individuals and organiza-
tions (pp. 127-156). London, England: Routledge.

Salas, E., & Fiore, S. M. (Eds.). (2004). Team cognition—Understanding the factors 
that drive process and performance. Washington, DC: American Psychological 
Association.

Salas, E., Fiore, S. M., & Letsky, M. P. (2012). Theories of team cognition: Cross-

disciplinary perspectives. New York, NY: Routledge.

Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater 
reliability. Psychological Bulletin, 86, 420-428. doi:10.1037/0033-2909.86.2.420
Srivastava,  A.,  Bartol,  K.  M.,  &  Locke,  E.  A.  (2006).  Empowering  leadership 
in  management  teams:  Effects  on  knowledge  sharing,  efficacy,  and  perfor-
mance.  The  Academy  of  Management  Journal,  49,  1239-1251.  doi:10.5465/
AMJ.2006.23478718

Stasser, G. (1992). Pooling of unshared information during group discussion. In S. 
Worchel, W. Wood, & J. Simpson (Eds.), Group process and productivity (pp. 
48-57). Newbury Park, CA: SAGE.

Stasser,  G.,  Stewart,  D.  D.,  &  Wittenbaum,  G.  M.  (1995).  Expert  roles  and  infor-
mation  exchange  during  discussion:  The  importance  of  knowing  who  knows 
what.  Journal  of  Experimental  Social  Psychology,  31,  244-265.  doi:10.1006/
jesp.1995.1012

Stout, R. J., Cannon-Bowers, J. A., Salas, E., & Milanovich, D. M. (1999). Planning, 
shared mental models, and coordinated performance: An empirical link is estab-
lished. Human Factors, 41, 61-71. doi:10.1518/001872099779577273

Tabachnick,  B.  G.,  &  Fidell,  L.  S.  (2001).  Using  multivariate  statistics  (4th  ed.). 

Needham Heights, MA: Allyn & Bacon.

Wageman,  R.  (1995).  Interdependence  and  group  effectiveness.  Administrative 

Science Quarterly, 40, 145-181.

Webber,  S.  S.,  Chen,  G.,  Payne,  S.  C.,  Marsh,  S.  M.,  &  Zaccaro,  S.  J.  (2000). 
Enhancing team mental model measurement with performance appraisal practices. 
Organizational Research Methods, 3, 307-322. doi:10.1177/109442810034001

Wegner, D. M. (1986). Transactive memory: A contemporary analysis of the group 
mind. In B. Mullen & G. R. Goethals (Eds.), Theories of group behavior (pp. 
185-205). New York, NY: Springer-Verlag.

Wegner, D. M. (1995). A computer network model of human transactive memory. 

Social Cognition, 13, 319-339. doi:10.1521/soco.1995.13.3.319

Wiedow,  A.,  Konradt,  U.,  Ellwart,  T.,  &  Steenfatt,  C.  (2013).  Direct  and  indirect 
effects of team process improvement on team outcomes: A multiple mediation 
analysis. Group Dynamics: Theory, Research, and Practice., 17, 232-251.

Author Biographies
Thomas Ellwart is a full professor in business psychology at the University of Trier, 
Germany. His current research interests include diversity in teams, team mental mod-
els, and team learning.

Ellwart et al. 

153

Udo Konradt holds a PhD in psychology and is a full professor of work, organiza-
tional  and  consumer  psychology  at  the  University  of  Kiel,  Germany.  His  research 
interests include virtual teams and virtual collaboration, IT usage and usability, and 
team learning.

Oliver Rack holds a PhD in psychology and is a professor of organizational psychol-
ogy at the University of Applied Sciences and Arts Northwestern Switzerland, School 
of Applied Psychology. His research interests include interaction processes in com-
puter-mediated  groups,  team  mental  models,  and  the  management  of  distributed 
collaboration.

